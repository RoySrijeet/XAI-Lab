{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB15k Job Default Config\n",
    "\n",
    "TASK='fb15k'\n",
    "NUM_VOCAB=16396  #NUM_VOCAB and NUM_RELATIONS must be consistent with vocab.txt file \n",
    "NUM_RELATIONS=1345\n",
    "\n",
    "# training hyper-paramters\n",
    "BATCH_SIZE=512\n",
    "LEARNING_RATE=5e-4\n",
    "EPOCH=400\n",
    "SOFT_LABEL=0.8\n",
    "SKIP_STEPS=1000\n",
    "MAX_SEQ_LEN=3\n",
    "HIDDEN_DROPOUT_PROB=0.1\n",
    "ATTENTION_PROBS_DROPOUT_PROB=0.1\n",
    "\n",
    "# file paths for training and evaluation \n",
    "DATA=\"./data\"\n",
    "OUTPUT=\"./output_\"+ TASK\n",
    "TRAIN_FILE= DATA + TASK + \"/train.coke.txt\"\n",
    "VALID_FILE=DATA + TASK + \"/valid.coke.txt\"\n",
    "TEST_FILE=DATA + TASK + \"/test.coke.txt\"\n",
    "VOCAB_PATH=DATA + TASK + \"/vocab.txt\"\n",
    "TRUE_TRIPLE_PATH=DATA + TASK + \"/all.txt\"\n",
    "CHECKPOINTS= OUTPUT + \"/models\"\n",
    "INIT_CHECKPOINTS= CHECKPOINTS\n",
    "LOG_FILE=OUTPUT+\"/train.log\"\n",
    "LOG_EVAL_FILE=OUTPUT+\"/test.log\"\n",
    "\n",
    "# transformer net config, the follwoing are default configs for all tasks\n",
    "HIDDEN_SIZE=256\n",
    "NUM_HIDDEN_LAYERS=12\n",
    "NUM_ATTENTION_HEADS=4\n",
    "MAX_POSITION_EMBEDDINGS=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset': TASK,\n",
    "    'vocab_size' : NUM_VOCAB,\n",
    "    'num_relations': NUM_RELATIONS,\n",
    "    \n",
    "    'use_cuda': False,\n",
    "    'do_train': True,\n",
    "    'do_predict': False,\n",
    "    'use_ema': False,\n",
    "    'use_fast_executor': False,\n",
    "    'num_iteration_per_drop_scope': 1,\n",
    "    \n",
    "    'train_file': TRAIN_FILE,\n",
    "    'true_triple_path': TRUE_TRIPLE_PATH,\n",
    "    'vocab_path': VOCAB_PATH,\n",
    "    'sen_candli_file': None, \n",
    "    'sen_trivial_file': None,\n",
    "    'predict_file': None,\n",
    "    \"in_tokens\": False,\n",
    "    \n",
    "    'max_seq_len':MAX_SEQ_LEN,\n",
    "    'checkpoints':CHECKPOINTS,\n",
    "    'soft_label': SOFT_LABEL,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epoch': EPOCH,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'skip_steps': SKIP_STEPS,\n",
    "    'hidden_dropout_prob': HIDDEN_DROPOUT_PROB,\n",
    "    'attention_probs_dropout_prob':ATTENTION_PROBS_DROPOUT_PROB,\n",
    "    \n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_hidden_layers': NUM_HIDDEN_LAYERS,\n",
    "    'num_attention_heads':NUM_ATTENTION_HEADS,\n",
    "    'max_position_embeddings':MAX_POSITION_EMBEDDINGS,\n",
    "    \n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"initializer_range\": 0.02, \n",
    "    \"intermediate_size\": 512,  \n",
    "    \"init_checkpoint\":  None,\n",
    "    \"init_pretraining_params\":  None, \n",
    "    \"weight_sharing\": True,\n",
    "    \n",
    "    \"lr_scheduler\": \"linear_warmup_decay\",\n",
    "    \"weight_decay\": 0.01, \n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"ema_decay\": 0.9999,\n",
    "    \"use_fp16\": False, \n",
    "    \"loss_scaling\": 1.0,\n",
    "    \n",
    "    \"skip_steps\": 1000,\n",
    "    \"verbose\": False,\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srijeet Roy\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "06/14/2022 11:58:04 - INFO - reader.coke_reader -   10\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "if paddle.__version__.startswith('2.'):\n",
    "    paddle.enable_static() # into static mode\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "from reader.coke_reader import KBCDataReader\n",
    "from reader.coke_reader import PathqueryDataReader\n",
    "from model.coke import CoKEModel\n",
    "from optimization import optimization\n",
    "#from evaluation import kbc_evaluation\n",
    "from evaluation import kbc_batch_evaluation\n",
    "from evaluation import compute_kbc_metrics\n",
    "from evaluation import pathquery_batch_evaluation\n",
    "from evaluation import compute_pathquery_metrics\n",
    "from utils.args import ArgumentGroup, print_arguments\n",
    "from utils.init import init_pretraining_params, init_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple\n",
      "Collecting paddlepaddle==2.3.0\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/01/37/040347acdd4683bbe45a914bf2321f261f378a902822e2cf6cd3b7265cce/paddlepaddle-2.3.0-cp38-cp38-win_amd64.whl (64.2 MB)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (3.19.3)\n",
      "Collecting paddle-bfloat==0.1.2\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/9b/b9/764f50d1c7dd242e61f378aea838aa67d64013c399ff7ccd6a11284de082/paddle_bfloat-0.1.2-cp38-cp38-win_amd64.whl (40 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (4.4.2)\n",
      "Collecting astor\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (1.19.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (2020.6.20)\n",
      "Installing collected packages: paddle-bfloat, astor, paddlepaddle\n",
      "Successfully installed astor-0.8.1 paddle-bfloat-0.1.2 paddlepaddle-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlepaddle==2.3.0 -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(pyreader_name, coke_config):\n",
    "    pyreader = fluid.layers.py_reader\\\n",
    "            (\n",
    "        capacity=50,\n",
    "        shapes=[[-1, args.max_seq_len, 1],\n",
    "                [-1, args.max_seq_len, 1],\n",
    "                [-1, args.max_seq_len, 1], [-1, 1], [-1, 1]],\n",
    "        dtypes=[\n",
    "            'int64', 'int64', 'float32', 'int64', 'int64'],\n",
    "        lod_levels=[0, 0, 0, 0, 0],\n",
    "        name=pyreader_name,\n",
    "        use_double_buffer=True)\n",
    "    (src_ids, pos_ids, input_mask, mask_labels, mask_positions) = fluid.layers.read_file(pyreader)\n",
    "\n",
    "    coke = CoKEModel(\n",
    "        src_ids=src_ids,\n",
    "        position_ids=pos_ids,\n",
    "        input_mask=input_mask,\n",
    "        config=coke_config,\n",
    "        soft_label=args.soft_label,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        use_fp16=args.use_fp16)\n",
    "\n",
    "    loss, fc_out = coke.get_pretraining_output(mask_label=mask_labels, mask_pos=mask_positions)\n",
    "    if args.use_fp16 and args.loss_scaling > 1.0:\n",
    "        loss = loss * args.loss_scaling\n",
    "\n",
    "    batch_ones = fluid.layers.fill_constant_batch_size_like(\n",
    "        input=mask_labels, dtype='int64', shape=[1], value=1)\n",
    "    num_seqs = fluid.layers.reduce_sum(input=batch_ones)\n",
    "\n",
    "    return pyreader, loss, fc_out, num_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coke_net_config(args, print_config = True):\n",
    "    \n",
    "    config = {}\n",
    "    config[\"hidden_size\"] = args[\"hidden_size\"]\n",
    "    config[\"num_hidden_layers\"] = args[\"num_hidden_layers\"]\n",
    "    config[\"num_attention_heads\"] = args[\"num_attention_heads\"]\n",
    "    config[\"vocab_size\"] = args[\"vocab_size\"]\n",
    "    config[\"num_relations\"] = args[\"num_relations\"]\n",
    "    config[\"max_position_embeddings\"] = args[\"max_position_embeddings\"]\n",
    "    config[\"hidden_act\"] = args[\"hidden_act\"]\n",
    "    config[\"hidden_dropout_prob\"] = args[\"hidden_dropout_prob\"]\n",
    "    config[\"attention_probs_dropout_prob\"] = args[\"attention_probs_dropout_prob\"]\n",
    "    config[\"initializer_range\"] = args[\"initializer_range\"]\n",
    "    config[\"intermediate_size\"] = args[\"intermediate_size\"]\n",
    "    \n",
    "    if print_config is True:\n",
    "        for arg, value in config.items():\n",
    "            print(f\"{arg}: {value}\")\n",
    "            \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/14/2022 11:29:30 - INFO - bin.reader.coke_reader -   10\n"
     ]
    }
   ],
   "source": [
    "from reader.coke_reader import KBCDataReader\n",
    "\n",
    "def get_data_reader(args, data_file, epoch, is_training, shuffle, vocab_size):\n",
    "    Reader = KBCDataReader\n",
    "    data_reader = Reader(vocab_path=args[\"vocab_path\"], \n",
    "                         data_path=data_file,\n",
    "                         max_seq_len=args[\"max_seq_len\"],\n",
    "                         batch_size=args[\"batch_size\"],\n",
    "                         is_training=is_training,\n",
    "                         shuffle=shuffle,\n",
    "                         epoch=epoch,\n",
    "                         dev_count=1,\n",
    "                         vocab_size=vocab_size)\n",
    "    return data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 256\n",
      "num_hidden_layers: 12\n",
      "num_attention_heads: 4\n",
      "vocab_size: 16396\n",
      "num_relations: 1345\n",
      "max_position_embeddings: 40\n",
      "hidden_act: gelu\n",
      "hidden_dropout_prob: 0.1\n",
      "attention_probs_dropout_prob: 0.1\n",
      "initializer_range: 0.02\n",
      "intermediate_size: 512\n"
     ]
    }
   ],
   "source": [
    "coke_config = init_coke_net_config(args, print_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['do_train']:\n",
    "    train_data_reader = get_data_reader(args, args[\"train_file\"], is_training=True,\n",
    "                                          epoch=args[\"epoch\"], shuffle=True, dev_count=1,\n",
    "                                          vocab_size=args[\"vocab_size\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
