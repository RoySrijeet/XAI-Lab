{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB15k Job Default Config\n",
    "\n",
    "TASK='fb15k'\n",
    "NUM_VOCAB=16396  #NUM_VOCAB and NUM_RELATIONS must be consistent with vocab.txt file \n",
    "NUM_RELATIONS=1345\n",
    "\n",
    "# training hyper-paramters\n",
    "BATCH_SIZE=512\n",
    "LEARNING_RATE=5e-4\n",
    "EPOCH=400\n",
    "SOFT_LABEL=0.8\n",
    "SKIP_STEPS=1000\n",
    "MAX_SEQ_LEN=3\n",
    "HIDDEN_DROPOUT_PROB=0.1\n",
    "ATTENTION_PROBS_DROPOUT_PROB=0.1\n",
    "\n",
    "# file paths for training and evaluation \n",
    "DATA=\"./data/\"\n",
    "OUTPUT=\"./output_\"+ TASK\n",
    "TRAIN_FILE= DATA + TASK + \"/train.coke.txt\"\n",
    "VALID_FILE=DATA + TASK + \"/valid.coke.txt\"\n",
    "TEST_FILE=DATA + TASK + \"/test.coke.txt\"\n",
    "VOCAB_PATH=DATA + TASK + \"/vocab.txt\"\n",
    "TRUE_TRIPLE_PATH=DATA + TASK + \"/all.txt\"\n",
    "CHECKPOINTS= OUTPUT + \"/models\"\n",
    "INIT_CHECKPOINTS= CHECKPOINTS\n",
    "LOG_FILE=OUTPUT+\"/train.log\"\n",
    "LOG_EVAL_FILE=OUTPUT+\"/test.log\"\n",
    "\n",
    "# transformer net config, the follwoing are default configs for all tasks\n",
    "HIDDEN_SIZE=256\n",
    "NUM_HIDDEN_LAYERS=12\n",
    "NUM_ATTENTION_HEADS=4\n",
    "MAX_POSITION_EMBEDDINGS=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset': TASK,\n",
    "    'vocab_size' : NUM_VOCAB,\n",
    "    'num_relations': NUM_RELATIONS,\n",
    "    \n",
    "    'use_cuda': False,\n",
    "    'do_train': True,\n",
    "    'do_predict': False,\n",
    "    'use_ema': False,\n",
    "    'use_fast_executor': False,\n",
    "    'num_iteration_per_drop_scope': 1,\n",
    "    \n",
    "    'train_file': TRAIN_FILE,\n",
    "    'true_triple_path': TRUE_TRIPLE_PATH,\n",
    "    'vocab_path': VOCAB_PATH,\n",
    "    'sen_candli_file': None, \n",
    "    'sen_trivial_file': None,\n",
    "    'predict_file': None,\n",
    "    \"in_tokens\": False,\n",
    "    \n",
    "    'max_seq_len':MAX_SEQ_LEN,\n",
    "    'checkpoints':CHECKPOINTS,\n",
    "    'soft_label': SOFT_LABEL,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epoch': EPOCH,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'skip_steps': SKIP_STEPS,\n",
    "    'hidden_dropout_prob': HIDDEN_DROPOUT_PROB,\n",
    "    'attention_probs_dropout_prob':ATTENTION_PROBS_DROPOUT_PROB,\n",
    "    \n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_hidden_layers': NUM_HIDDEN_LAYERS,\n",
    "    'num_attention_heads':NUM_ATTENTION_HEADS,\n",
    "    'max_position_embeddings':MAX_POSITION_EMBEDDINGS,\n",
    "    \n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"initializer_range\": 0.02, \n",
    "    \"intermediate_size\": 512,  \n",
    "    \"init_checkpoint\":  None,\n",
    "    \"init_pretraining_params\":  None, \n",
    "    \"weight_sharing\": True,\n",
    "    \n",
    "    \"lr_scheduler\": \"linear_warmup_decay\",\n",
    "    \"weight_decay\": 0.01, \n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"ema_decay\": 0.9999,\n",
    "    \"use_fp16\": False, \n",
    "    \"loss_scaling\": 1.0,\n",
    "    \n",
    "    \"skip_steps\": 1000,\n",
    "    \"verbose\": False,\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/21/2022 15:36:35 - INFO - reader.coke_reader -   10\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "if paddle.__version__.startswith('2.'):\n",
    "    paddle.enable_static() # into static mode\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "from reader.coke_reader import KBCDataReader\n",
    "from reader.coke_reader import PathqueryDataReader\n",
    "from model.coke import CoKEModel\n",
    "from optimization import optimization\n",
    "#from evaluation import kbc_evaluation\n",
    "from evaluation import kbc_batch_evaluation\n",
    "from evaluation import compute_kbc_metrics\n",
    "from evaluation import pathquery_batch_evaluation\n",
    "from evaluation import compute_pathquery_metrics\n",
    "from utils.args import ArgumentGroup, print_arguments\n",
    "from utils.init import init_pretraining_params, init_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple\n",
      "Collecting paddlepaddle==2.3.0\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/01/37/040347acdd4683bbe45a914bf2321f261f378a902822e2cf6cd3b7265cce/paddlepaddle-2.3.0-cp38-cp38-win_amd64.whl (64.2 MB)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (3.19.3)\n",
      "Collecting paddle-bfloat==0.1.2\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/9b/b9/764f50d1c7dd242e61f378aea838aa67d64013c399ff7ccd6a11284de082/paddle_bfloat-0.1.2-cp38-cp38-win_amd64.whl (40 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (4.4.2)\n",
      "Collecting astor\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from paddlepaddle==2.3.0) (1.19.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srijeet roy\\anaconda3\\lib\\site-packages (from requests>=2.20.0->paddlepaddle==2.3.0) (2020.6.20)\n",
      "Installing collected packages: paddle-bfloat, astor, paddlepaddle\n",
      "Successfully installed astor-0.8.1 paddle-bfloat-0.1.2 paddlepaddle-2.3.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install paddlepaddle==2.3.0 -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srijeet Roy\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    level=logging.INFO)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(pyreader_name, coke_config):\n",
    "    pyreader = fluid.layers.py_reader\\\n",
    "            (\n",
    "        capacity=50,\n",
    "        shapes=[[-1, args[\"max_seq_len\"], 1],\n",
    "                [-1, args[\"max_seq_len\"], 1],\n",
    "                [-1, args[\"max_seq_len\"], 1], [-1, 1], [-1, 1]],\n",
    "        dtypes=[\n",
    "            'int64', 'int64', 'float32', 'int64', 'int64'],\n",
    "        lod_levels=[0, 0, 0, 0, 0],\n",
    "        name=pyreader_name,\n",
    "        use_double_buffer=True)\n",
    "    (src_ids, pos_ids, input_mask, mask_labels, mask_positions) = fluid.layers.read_file(pyreader)\n",
    "\n",
    "    coke = CoKEModel(\n",
    "        src_ids=src_ids,\n",
    "        position_ids=pos_ids,\n",
    "        input_mask=input_mask,\n",
    "        config=coke_config,\n",
    "        soft_label=args[\"soft_label\"],\n",
    "        weight_sharing=args[\"weight_sharing\"],\n",
    "        use_fp16=args[\"use_fp16\"])\n",
    "\n",
    "    loss, fc_out = coke.get_pretraining_output(mask_label=mask_labels, mask_pos=mask_positions)\n",
    "    if args[\"use_fp16\"] and args[\"loss_scaling\"] > 1.0:\n",
    "        loss = loss * args[\"loss_scaling\"]\n",
    "\n",
    "    batch_ones = fluid.layers.fill_constant_batch_size_like(\n",
    "        input=mask_labels, dtype='int64', shape=[1], value=1)\n",
    "    num_seqs = fluid.layers.reduce_sum(input=batch_ones)\n",
    "\n",
    "    return pyreader, loss, fc_out, num_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbc_predict(test_exe, test_program, test_pyreader, fetch_list, all_examples, true_triplets_dict, eval_result_file):\n",
    "    eval_i = 0\n",
    "    step = 0\n",
    "    batch_eval_rets = []\n",
    "    f_batch_eval_rets = []\n",
    "    test_pyreader.start()\n",
    "    while True:\n",
    "        try:\n",
    "            batch_results = []\n",
    "            np_fc_out = test_exe.run(fetch_list=fetch_list, program=test_program)[0]\n",
    "            _batch_len = np_fc_out.shape[0]\n",
    "            for idx in range(np_fc_out.shape[0]):\n",
    "                logits = [float(x) for x in np_fc_out[idx].flat]\n",
    "                batch_results.append(logits)\n",
    "            rank, frank = kbc_batch_evaluation(eval_i, all_examples, batch_results, true_triplets_dict)\n",
    "            batch_eval_rets.extend(rank)\n",
    "            f_batch_eval_rets.extend(frank)\n",
    "            if step % 10 == 0:\n",
    "                logger.info(\"Processing kbc_predict step: %d exmaples:%d\" % (step, eval_i))\n",
    "            step += 1\n",
    "            eval_i += _batch_len\n",
    "        except fluid.core.EOFException:\n",
    "            test_pyreader.reset()\n",
    "            break\n",
    "    eval_result = compute_kbc_metrics(batch_eval_rets, f_batch_eval_rets, eval_result_file)\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_exe, test_program, test_pyreader, fetch_list, all_examples, args):\n",
    "    dataset = args[\"dataset\"]\n",
    "    if not os.path.exists(args[\"checkpoints\"]):\n",
    "        os.makedirs(args[\"checkpoints\"])\n",
    "    eval_result_file = os.path.join(args[\"checkpoints\"], \"eval_result.json\")\n",
    "    logger.info(\">> Evaluation result file: %s\" % eval_result_file)\n",
    "\n",
    "    if dataset.lower() in [\"pathquerywn\", \"pathqueryfb\"]:\n",
    "        sen_candli_dict, trivial_sen_set = _load_pathquery_eval_dict(args[\"sen_candli_file\"],\n",
    "                                                                   args[\"sen_trivial_file\"])\n",
    "        logger.debug(\">> Load sen_candli_dict size: %d\" % len(sen_candli_dict))\n",
    "        logger.debug(\">> Trivial sen set size: %d\" % len(trivial_sen_set))\n",
    "        logger.debug(\">> Finish load sen_candli set at:{}\".format(time.ctime()))\n",
    "        eval_performance = pathquery_predict(test_exe, test_program, test_pyreader, fetch_list,\n",
    "                                              all_examples, sen_candli_dict, trivial_sen_set,\n",
    "                                              eval_result_file)\n",
    "\n",
    "        outs = \"%s\\t%.3f\\t%.3f\" % (args[\"dataset\"], eval_performance['mq'], eval_performance['fhits10'])\n",
    "        logger.info(\"\\n---------- Evaluation Performance --------------\\n%s\\n%s\" %\n",
    "                    (\"\\t\".join([\"TASK\", \"MQ\", \"Hits@10\"]), outs))\n",
    "    else:\n",
    "        true_triplets_dict = _load_kbc_eval_dict(args[\"true_triple_path\"])\n",
    "        logger.info(\">> Finish loading true triplets dict %s\" % time.ctime())\n",
    "        eval_performance = kbc_predict(test_exe, test_program, test_pyreader, fetch_list,\n",
    "                                        all_examples, true_triplets_dict, eval_result_file)\n",
    "        outs = \"%s\\t%.3f\\t%.3f\\t%.3f\\t%.3f\" % (args[\"dataset\"],\n",
    "                                               eval_performance['fmrr'],\n",
    "                                               eval_performance['fhits1'],\n",
    "                                               eval_performance['fhits3'],\n",
    "                                               eval_performance['fhits10'])\n",
    "        logger.info(\"\\n----------- Evaluation Performance --------------\\n%s\\n%s\" %\n",
    "                    (\"\\t\".join([\"TASK\", \"MRR\", \"Hits@1\", \"Hits@3\", \"Hits@10\"]), outs))\n",
    "    return eval_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_kbc_eval_dict(true_triple_file):\n",
    "    def load_true_triples(true_triple_file):\n",
    "        true_triples = []\n",
    "        with open(true_triple_file, \"r\") as fr:\n",
    "            for line in fr.readlines():\n",
    "                tokens = line.strip(\"\\r \\n\").split(\"\\t\")\n",
    "                assert len(tokens) == 3\n",
    "                true_triples.append(\n",
    "                    (int(tokens[0]), int(tokens[1]), int(tokens[2])))\n",
    "        logger.debug(\"Finish loading %d true triples\" % len(true_triples))\n",
    "        return true_triples\n",
    "    true_triples = load_true_triples(true_triple_file)\n",
    "    true_triples_dict = collections.defaultdict(lambda: {'hs': collections.defaultdict(list),\n",
    "                                          'ts': collections.defaultdict(list)})\n",
    "    for h, r, t in true_triples:\n",
    "        true_triples_dict[r]['ts'][h].append(t)\n",
    "        true_triples_dict[r]['hs'][t].append(h)\n",
    "    return true_triples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coke_net_config(args, print_config = True):\n",
    "    \n",
    "    config = {}\n",
    "    config[\"hidden_size\"] = args[\"hidden_size\"]\n",
    "    config[\"num_hidden_layers\"] = args[\"num_hidden_layers\"]\n",
    "    config[\"num_attention_heads\"] = args[\"num_attention_heads\"]\n",
    "    config[\"vocab_size\"] = args[\"vocab_size\"]\n",
    "    config[\"num_relations\"] = args[\"num_relations\"]\n",
    "    config[\"max_position_embeddings\"] = args[\"max_position_embeddings\"]\n",
    "    config[\"hidden_act\"] = args[\"hidden_act\"]\n",
    "    config[\"hidden_dropout_prob\"] = args[\"hidden_dropout_prob\"]\n",
    "    config[\"attention_probs_dropout_prob\"] = args[\"attention_probs_dropout_prob\"]\n",
    "    config[\"initializer_range\"] = args[\"initializer_range\"]\n",
    "    config[\"intermediate_size\"] = args[\"intermediate_size\"]\n",
    "    \n",
    "    \n",
    "    if print_config is True:\n",
    "        logger.info('----------- CoKE Network Configuration -------------')\n",
    "        for arg, value in config.items():\n",
    "            logger.info('%s: %s' % (arg, value))\n",
    "        logger.info('------------------------------------------------')\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_predict_checkpoint(args, exe, startup_prog):\n",
    "    if args[\"dataset\"] in [\"pathQueryWN\", \"pathQueryFB\"]:\n",
    "        assert args[\"sen_candli_file\"] is not None and args[\"sen_trivial_file\"] is not None, \"during test, pathQuery sen_candli_file and path_trivial_file must be set \"\n",
    "    if not args[\"init_checkpoint\"]:\n",
    "        raise ValueError(\"args 'init_checkpoint' should be set if\"\n",
    "                         \"only doing prediction!\")\n",
    "    init_checkpoint(\n",
    "        exe,\n",
    "        args[\"init_checkpoint\"],\n",
    "        main_program=startup_prog,\n",
    "        use_fp16=args[\"use_fp16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_checkpoint(args, exe, startup_prog):\n",
    "    if args[\"init_checkpoint\"] and args[\"init_pretraining_params\"]:\n",
    "        logger.info(\n",
    "            \"WARNING: args 'init_checkpoint' and 'init_pretraining_params' \"\n",
    "            \"both are set! Only arg 'init_checkpoint' is made valid.\")\n",
    "    if args[\"init_checkpoint\"]:\n",
    "        init_checkpoint(\n",
    "            exe,\n",
    "            args[\"init_checkpoint\"],\n",
    "            main_program=startup_prog,\n",
    "            use_fp16=args[\"use_fp16\"],\n",
    "            print_var_verbose=False)\n",
    "    elif args[\"init_pretraining_params\"]:\n",
    "        init_pretraining_params(\n",
    "            exe,\n",
    "            args[\"init_pretraining_params\"],\n",
    "            main_program=startup_prog,\n",
    "            use_fp16=args[\"use_fp16\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader.coke_reader import KBCDataReader\n",
    "\n",
    "def get_data_reader(args, data_file, epoch, is_training, shuffle, dev_count, vocab_size):\n",
    "    if args[\"dataset\"].lower() in [\"pathqueryfb\", \"pathquerywn\"]:\n",
    "        Reader = PathqueryDataReader\n",
    "    else:\n",
    "        Reader = KBCDataReader\n",
    "    data_reader = Reader(vocab_path=args[\"vocab_path\"], \n",
    "                         data_path=data_file,\n",
    "                         max_seq_len=args[\"max_seq_len\"],\n",
    "                         batch_size=args[\"batch_size\"],\n",
    "                         is_training=is_training,\n",
    "                         shuffle=shuffle,\n",
    "                         epoch=epoch,\n",
    "                         dev_count=dev_count,\n",
    "                         vocab_size=vocab_size)\n",
    "    return data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 256\n",
      "num_hidden_layers: 12\n",
      "num_attention_heads: 4\n",
      "vocab_size: 16396\n",
      "num_relations: 1345\n",
      "max_position_embeddings: 40\n",
      "hidden_act: gelu\n",
      "hidden_dropout_prob: 0.1\n",
      "attention_probs_dropout_prob: 0.1\n",
      "initializer_range: 0.02\n",
      "intermediate_size: 512\n"
     ]
    }
   ],
   "source": [
    "# coke_config = init_coke_net_config(args, print_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args['do_train']:\n",
    "#     train_data_reader = get_data_reader(args, args[\"train_file\"], is_training=True,\n",
    "#                                           epoch=args[\"epoch\"], shuffle=True, dev_count=1,\n",
    "#                                           vocab_size=args[\"vocab_size\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if not (args[\"do_train\"] or args[\"do_predict\"]):\n",
    "        raise ValueError(\"For args `do_train` and `do_predict`, at \"\n",
    "                         \"least one of them must be True.\")\n",
    "    if args[\"use_cuda\"]:\n",
    "        place = fluid.CUDAPlace(0)\n",
    "        dev_count = fluid.core.get_cuda_device_count()\n",
    "    else:\n",
    "        place = fluid.CPUPlace()\n",
    "        dev_count = int(os.environ.get('CPU_NUM', multiprocessing.cpu_count()))\n",
    "    exe = fluid.Executor(place)\n",
    "\n",
    "    startup_prog = fluid.Program()\n",
    "\n",
    "    # Init programs\n",
    "    coke_config = init_coke_net_config(args, print_config=True)\n",
    "    if args[\"do_train\"]:\n",
    "        train_data_reader = get_data_reader(args, args[\"train_file\"], is_training=True,\n",
    "                                            epoch=args[\"epoch\"], shuffle=True, dev_count=dev_count,\n",
    "                                            vocab_size=args[\"vocab_size\"])\n",
    "\n",
    "        num_train_examples = train_data_reader.total_instance\n",
    "        if args[\"in_tokens\"]:\n",
    "            max_train_steps = args[\"epoch\"] * num_train_examples // (\n",
    "                    args[\"batch_size\"] // args[\"max_seq_len\"]) // dev_count\n",
    "        else:\n",
    "            max_train_steps = args[\"epoch\"] * num_train_examples // (\n",
    "                args[\"batch_size\"]) // dev_count\n",
    "        warmup_steps = int(max_train_steps * args[\"warmup_proportion\"])\n",
    "        logger.info(\"Device count: %d\" % dev_count)\n",
    "        logger.info(\"Num train examples: %d\" % num_train_examples)\n",
    "        logger.info(\"Max train steps: %d\" % max_train_steps)\n",
    "        logger.info(\"Num warmup steps: %d\" % warmup_steps)\n",
    "\n",
    "        train_program = fluid.Program()\n",
    "\n",
    "        # Create model and set optimization for train\n",
    "        with fluid.program_guard(train_program, startup_prog):\n",
    "            with fluid.unique_name.guard():\n",
    "                train_pyreader, loss, _, num_seqs = create_model(\n",
    "                    pyreader_name='train_reader',\n",
    "                    coke_config=coke_config)\n",
    "\n",
    "                scheduled_lr = optimization(\n",
    "                    loss=loss,\n",
    "                    warmup_steps=warmup_steps,\n",
    "                    num_train_steps=max_train_steps,\n",
    "                    learning_rate=args[\"learning_rate\"],\n",
    "                    train_program=train_program,\n",
    "                    startup_prog=startup_prog,\n",
    "                    weight_decay=args[\"weight_decay\"],\n",
    "                    scheduler=args[\"lr_scheduler\"],\n",
    "                    use_fp16=args[\"use_fp16\"],\n",
    "                    loss_scaling=args[\"loss_scaling\"])\n",
    "\n",
    "                if args[\"use_ema\"]:\n",
    "                    ema = fluid.optimizer.ExponentialMovingAverage(args[\"ema_decay\"])\n",
    "                    ema.update()\n",
    "\n",
    "                fluid.memory_optimize(train_program, skip_opt_set=[loss.name, num_seqs.name])\n",
    "\n",
    "        if args[\"verbose\"]:\n",
    "            if args[\"in_tokens\"]:\n",
    "                lower_mem, upper_mem, unit = fluid.contrib.memory_usage(\n",
    "                    program=train_program,\n",
    "                    batch_size=args[\"batch_size\"] // args[\"max_seq_len\"])\n",
    "            else:\n",
    "                lower_mem, upper_mem, unit = fluid.contrib.memory_usage(\n",
    "                    program=train_program, batch_size=args[\"batch_size\"])\n",
    "            logger.info(\"Theoretical memory usage in training:  %.3f - %.3f %s\" %\n",
    "                        (lower_mem, upper_mem, unit))\n",
    "\n",
    "    if args[\"do_predict\"]:\n",
    "        # Create model for prediction\n",
    "        test_prog = fluid.Program()\n",
    "        with fluid.program_guard(test_prog, startup_prog):\n",
    "            with fluid.unique_name.guard():\n",
    "                test_pyreader, _, fc_out, num_seqs = create_model(\n",
    "                    pyreader_name='test_reader',\n",
    "                    coke_config=coke_config)\n",
    "\n",
    "                if args[\"use_ema\"] and 'ema' not in dir():\n",
    "                    ema = fluid.optimizer.ExponentialMovingAverage(args[\"ema_decay\"])\n",
    "\n",
    "                fluid.memory_optimize(test_prog, skip_opt_set=[fc_out.name, num_seqs.name])\n",
    "\n",
    "        test_prog = test_prog.clone(for_test=True)\n",
    "\n",
    "    exe.run(startup_prog)\n",
    "\n",
    "    # Init checkpoints\n",
    "    if args[\"do_train\"]:\n",
    "        init_train_checkpoint(args, exe, startup_prog)\n",
    "    elif args[\"do_predict\"]:\n",
    "        init_predict_checkpoint(args, exe, startup_prog)\n",
    "\n",
    "    # Run training\n",
    "    if args[\"do_train\"]:\n",
    "        exec_strategy = fluid.ExecutionStrategy()\n",
    "        exec_strategy.use_experimental_executor = args[\"use_fast_executor\"]\n",
    "        exec_strategy.num_threads = dev_count\n",
    "        exec_strategy.num_iteration_per_drop_scope = args[\"num_iteration_per_drop_scope\"]\n",
    "\n",
    "        train_exe = fluid.ParallelExecutor(\n",
    "            use_cuda=args[\"use_cuda\"],\n",
    "            loss_name=loss.name,\n",
    "            exec_strategy=exec_strategy,\n",
    "            main_program=train_program)\n",
    "\n",
    "        train_pyreader.decorate_tensor_provider(train_data_reader.data_generator())\n",
    "\n",
    "        train_pyreader.start()\n",
    "        steps = 0\n",
    "        total_cost, total_num_seqs = [], []\n",
    "        time_begin = time.time()\n",
    "        while steps < max_train_steps:\n",
    "            try:\n",
    "                steps += 1\n",
    "                if steps % args[\"skip_steps\"] == 0:\n",
    "                    if warmup_steps <= 0:\n",
    "                        fetch_list = [loss.name, num_seqs.name]\n",
    "                    else:\n",
    "                        fetch_list = [\n",
    "                            loss.name, scheduled_lr.name, num_seqs.name\n",
    "                        ]\n",
    "                else:\n",
    "                    fetch_list = []\n",
    "\n",
    "                outputs = train_exe.run(fetch_list=fetch_list)\n",
    "\n",
    "                if steps % args[\"skip_steps\"] == 0:\n",
    "                    if warmup_steps <= 0:\n",
    "                        np_loss, np_num_seqs = outputs\n",
    "                    else:\n",
    "                        np_loss, np_lr, np_num_seqs = outputs\n",
    "                    total_cost.extend(np_loss * np_num_seqs)\n",
    "                    total_num_seqs.extend(np_num_seqs)\n",
    "\n",
    "                    if args[\"verbose\"]:\n",
    "                        verbose = \"train pyreader queue size: %d, \" % train_pyreader.queue.size(\n",
    "                        )\n",
    "                        verbose += \"learning rate: %f\" % (\n",
    "                            np_lr[0]\n",
    "                            if warmup_steps > 0 else args[\"learning_rate\"])\n",
    "                        logger.info(verbose)\n",
    "\n",
    "                    time_end = time.time()\n",
    "                    used_time = time_end - time_begin\n",
    "                    current_example, epoch = train_data_reader.get_progress()\n",
    "\n",
    "                    logger.info(\"epoch: %d, progress: %d/%d, step: %d, loss: %f, \"\n",
    "                                \"speed: %f steps/s\" %\n",
    "                                (epoch, current_example, num_train_examples, steps,\n",
    "                                 np.sum(total_cost) / np.sum(total_num_seqs),\n",
    "                                 args[\"skip_steps\"] / used_time))\n",
    "                    total_cost, total_num_seqs = [], []\n",
    "                    time_begin = time.time()\n",
    "\n",
    "                if steps == max_train_steps:\n",
    "                    save_path = os.path.join(args[\"checkpoints\"], \"step_\" + str(steps))\n",
    "                    fluid.io.save_persistables(exe, save_path, train_program)\n",
    "            except fluid.core.EOFException:\n",
    "                logger.warning(\">> EOFException\")\n",
    "                save_path = os.path.join(args.checkpoints, \"step_\" + str(steps) + \"_final\")\n",
    "                fluid.io.save_persistables(exe, save_path, train_program)\n",
    "                train_pyreader.reset()\n",
    "                break\n",
    "        logger.info(\">>Finish training at %s \" % time.ctime())\n",
    "\n",
    "    # Run prediction\n",
    "    if args[\"do_predict\"]:\n",
    "        assert dev_count == 1, \"During prediction, dev_count expects 1, current is %d\" % dev_count\n",
    "        test_data_reader = get_data_reader(args, args[\"predict_file\"], is_training=False,\n",
    "                                           epoch=1, shuffle=False, dev_count=dev_count,\n",
    "                                           vocab_size=args[\"vocab_size\"])\n",
    "        test_pyreader.decorate_tensor_provider(test_data_reader.data_generator())\n",
    "\n",
    "        if args[\"use_ema\"]:\n",
    "            with ema.apply(exe):\n",
    "                eval_performance = predict(exe, test_prog, test_pyreader,\n",
    "                                           [fc_out.name], test_data_reader.examples, args)\n",
    "        else:\n",
    "            eval_performance = predict(exe, test_prog, test_pyreader,\n",
    "                                       [fc_out.name], test_data_reader.examples, args)\n",
    "\n",
    "        logger.info(\">>Finish predicting at %s \" % time.ctime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/21/2022 15:40:44 - INFO - __main__ -   ----------- CoKE Network Configuration -------------\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   hidden_size: 256\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   num_hidden_layers: 12\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   num_attention_heads: 4\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   vocab_size: 16396\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   num_relations: 1345\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   max_position_embeddings: 40\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   hidden_act: gelu\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   hidden_dropout_prob: 0.1\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   attention_probs_dropout_prob: 0.1\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   initializer_range: 0.02\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   intermediate_size: 512\n",
      "06/21/2022 15:40:44 - INFO - __main__ -   ------------------------------------------------\n",
      "06/21/2022 15:40:50 - INFO - __main__ -   Device count: 8\n",
      "06/21/2022 15:40:50 - INFO - __main__ -   Num train examples: 966284\n",
      "06/21/2022 15:40:50 - INFO - __main__ -   Max train steps: 94363\n",
      "06/21/2022 15:40:50 - INFO - __main__ -   Num warmup steps: 9436\n",
      "06/21/2022 15:40:50 - WARNING - root -   paddle.fluid.layers.py_reader() may be deprecated in the near future. Please use paddle.fluid.io.DataLoader.from_generator() instead.\n",
      "06/21/2022 15:40:57 - WARNING - root -   Caution! paddle.fluid.memory_optimize() is deprecated and not maintained any more, since it is not stable!\n",
      "This API would not take any memory optimizations on your Program now, since we have provided default strategies for you.\n",
      "The newest and stable memory optimization strategies (they are all enabled by default) are as follows:\n",
      " 1. Garbage collection strategy, which is enabled by exporting environment variable FLAGS_eager_delete_tensor_gb=0 (0 is the default value).\n",
      " 2. Inplace strategy, which is enabled by setting build_strategy.enable_inplace=True (True is the default value) when using CompiledProgram or ParallelExecutor.\n",
      "\n",
      "!!! The CPU_NUM is not specified, you should set CPU_NUM in the environment variable list.\n",
      "CPU_NUM indicates that how many CPUPlace are used in the current task.\n",
      "And if this parameter are set as N (equal to the number of physical CPU core) the program may be faster.\n",
      "\n",
      "export CPU_NUM=8 # for example, set CPU_NUM as number of physical CPU core which is 8.\n",
      "\n",
      "!!! The default number of CPU_NUM=1.\n",
      "06/21/2022 16:18:43 - INFO - __main__ -   epoch: 0, progress: 540672/966284, step: 1000, loss: 8.676849, speed: 0.441548 steps/s\n",
      "06/21/2022 16:57:21 - INFO - __main__ -   epoch: 1, progress: 86388/966284, step: 2000, loss: 7.506804, speed: 0.431399 steps/s\n",
      "06/21/2022 17:59:51 - INFO - __main__ -   epoch: 1, progress: 598388/966284, step: 3000, loss: 7.039506, speed: 0.266692 steps/s\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
