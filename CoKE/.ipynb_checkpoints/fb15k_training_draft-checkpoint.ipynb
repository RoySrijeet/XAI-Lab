{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'dataset': 'fb15k',\n",
    "        'vocab_size' : 16396,\n",
    "        'num_relations': 1345,\n",
    "        # training hyper-paramters\n",
    "        'batch_size':512,\n",
    "        'learning_rate':5e-4,\n",
    "        'epoch':400,\n",
    "        'soft_label':0.8,\n",
    "        'skip_steps':1000,\n",
    "        'max_seq_len':3,\n",
    "        'hidden_dropout_prob':0.1,\n",
    "        'attention_probs_dropout_prob':0.1,\n",
    "        # file paths for training and evaluation \n",
    "        'data':\"./data\",\n",
    "        # OUTPUT=\"./output_\"+dataset\n",
    "        'train_file':\"./data/train.coke.txt\",\n",
    "        # VALID_FILE=\"./data/valid.coke.txt\"\n",
    "        # TEST_FILE=\"./data/test.coke.txt\"\n",
    "        'vocab_path':\"./data/vocab.txt\",\n",
    "        'true_triple_path':\"./data/all.txt\",\n",
    "        # transformer config, the follwoing are default configs for all tasks\n",
    "        'hidden_size':256,\n",
    "        'num_hidden_layers':12,\n",
    "        'num_attention_heads':4,\n",
    "        'max_position_embeddings':40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from bin.utils.args import ArgumentGroup, print_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "model_g = ArgumentGroup(parser, \"model\", \"model configuration and paths.\")\n",
    "model_g.add_arg(\"hidden_size\",              int, 256,            \"CoKE model config: hidden size, default 256\")\n",
    "model_g.add_arg(\"num_hidden_layers\",        int, 6,              \"CoKE model config: num_hidden_layers, default 6\")\n",
    "model_g.add_arg(\"num_attention_heads\",      int, 4,              \"CoKE model config: num_attention_heads, default 4\")\n",
    "model_g.add_arg(\"vocab_size\",               int, -1,           \"CoKE model config: vocab_size\")\n",
    "model_g.add_arg(\"num_relations\",         int, None,           \"CoKE model config: vocab_size\")\n",
    "model_g.add_arg(\"max_position_embeddings\",  int, 10,             \"CoKE model config: max_position_embeddings\")\n",
    "model_g.add_arg(\"hidden_act\",               str, \"gelu\",         \"CoKE model config: hidden_ac, default gelu\")\n",
    "model_g.add_arg(\"hidden_dropout_prob\",      float, 0.1,          \"CoKE model config: attention_probs_dropout_prob, default 0.1\")\n",
    "model_g.add_arg(\"attention_probs_dropout_prob\", float, 0.1,      \"CoKE model config: attention_probs_dropout_prob, default 0.1\")\n",
    "model_g.add_arg(\"initializer_range\",        int, 0.02,           \"CoKE model config: initializer_range\")\n",
    "model_g.add_arg(\"intermediate_size\",        int, 512,            \"CoKE model config: intermediate_size, default 512\")\n",
    "model_g.add_arg(\"init_checkpoint\",          str,  None,          \"Init checkpoint to resume training from, or for prediction only\")\n",
    "model_g.add_arg(\"init_pretraining_params\",  str,  None,          \"Init pre-training params which preforms fine-tuning from. If the \"\n",
    "                 \"arg 'init_checkpoint' has been set, this argument wouldn't be valid.\")\n",
    "model_g.add_arg(\"checkpoints\",              str,  \"checkpoints\", \"Path to save checkpoints.\")\n",
    "model_g.add_arg(\"weight_sharing\",           bool, True,          \"If set, share weights between word embedding and masked lm.\")\n",
    "\n",
    "train_g = ArgumentGroup(parser, \"training\", \"training options.\")\n",
    "train_g.add_arg(\"epoch\",             int,    100,                \"Number of epoches for training.\")\n",
    "train_g.add_arg(\"learning_rate\",     float,  5e-5,               \"Learning rate used to train with warmup.\")\n",
    "train_g.add_arg(\"lr_scheduler\",     str, \"linear_warmup_decay\",  \"scheduler of learning rate.\",\n",
    "                choices=['linear_warmup_decay', 'noam_decay'])\n",
    "train_g.add_arg(\"soft_label\",               float, 0.9,          \"Value of soft labels for loss computation\")\n",
    "train_g.add_arg(\"weight_decay\",      float,  0.01,               \"Weight decay rate for L2 regularizer.\")\n",
    "train_g.add_arg(\"warmup_proportion\", float,  0.1,                \"Proportion of training steps to perform linear learning rate warmup for.\")\n",
    "train_g.add_arg(\"use_ema\",           bool,   True,               \"Whether to use ema.\")\n",
    "train_g.add_arg(\"ema_decay\",         float,  0.9999,             \"Decay rate for expoential moving average.\")\n",
    "train_g.add_arg(\"use_fp16\",          bool,   False,              \"Whether to use fp16 mixed precision training.\")\n",
    "train_g.add_arg(\"loss_scaling\",      float,  1.0,                \"Loss scaling factor for mixed precision training, only valid when use_fp16 is enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/09/2022 22:51:27 - INFO - bin.utils.args - -----------  Configuration Arguments -----------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-14a4b859c0e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Workspace\\_RWTH\\_msc_mi\\SoSe 2022\\XAI\\CoKE\\CoKE\\bin\\utils\\args.py\u001b[0m in \u001b[0;36mprint_arguments\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----------  Configuration Arguments -----------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "print_arguments(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
