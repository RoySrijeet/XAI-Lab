{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"fb15k\", \"fb15k237\", \"pathqueryFB\", \"pathqueryWN\", \"wn18\", \"wn18rr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing KBC Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the unique entities and relations across all the 3 data files. This is to be later used to build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_entities_relations(train, valid, test):\n",
    "    print(\"[INFO] Extracting entities and relations...\")\n",
    "    entity_list = set()\n",
    "    relation_list = set()\n",
    "    \n",
    "    for input_file in [train, valid, test]:\n",
    "        filename = input_file.split('\\\\')\n",
    "        print(f\"[INFO] Working with {filename[-1]} file\")\n",
    "        \n",
    "        # tab-separated (head, relation, tail) triples\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                tokens = line.strip().split(\"\\t\")\n",
    "                assert len(tokens) == 3\n",
    "                entity_list.add(tokens[0])\n",
    "                entity_list.add(tokens[2])\n",
    "                relation_list.add(tokens[1])\n",
    "\n",
    "    return entity_list, relation_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vocabulary with the extracted entities and relations. Write it to the disc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(vocabulary, entity_list, relation_list):\n",
    "    print(\"[INFO] Creating vocabulary...\")\n",
    "    fout = open(vocabulary, \"w\")\n",
    "    fout.write(\"[PAD]\" + \"\\n\")\n",
    "    for i in range(95):\n",
    "        fout.write(f\"[unused{i}]\\n\")\n",
    "    fout.write(\"[UNK]\" + \"\\n\")\n",
    "    fout.write(\"[CLS]\" + \"\\n\")\n",
    "    fout.write(\"[SEP]\" + \"\\n\")\n",
    "    fout.write(\"[MASK]\" + \"\\n\")\n",
    "    for e in entity_list:\n",
    "        fout.write(e + \"\\n\")\n",
    "    for r in relation_list:\n",
    "        fout.write(r + \"\\n\")\n",
    "    vocab_size = 100 + len(entity_list) + len(relation_list)\n",
    "    print(f\"[INFO] Vocabulary size {vocab_size}\")\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the vocabulary as an ordered dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    fin = open(vocab_file)\n",
    "    for num, line in enumerate(fin):\n",
    "        token = line.strip()\n",
    "        index = num\n",
    "        vocab[token] = int(index)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the relation triples using the (indexed) vocabulary.\n",
    "\n",
    "For each of the training, validation and test files, read each raw triple.\n",
    "Replace the elements of the triple with their corresponding index in the vocabulary dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_true_triples(train, valid, test, vocab, output_file):\n",
    "    print(\"[INFO] Writing down all the triples...\")\n",
    "    true_triples = []\n",
    "    for input_file in [train, valid, test]:\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                h, r, t = line.strip('\\r \\n').split('\\t')\n",
    "                assert (h in vocab) and (r in vocab) and (t in vocab)\n",
    "                hpos = vocab[h]\n",
    "                rpos = vocab[r]\n",
    "                tpos = vocab[t]\n",
    "                true_triples.append((hpos, rpos, tpos))\n",
    "    \n",
    "    print(f\"[INFO] Number of true triples: {len(true_triples)}\")\n",
    "    fout = open(output_file, \"w\")\n",
    "    for hpos, rpos, tpos in true_triples:\n",
    "        fout.write(str(hpos) + \"\\t\" + str(rpos) + \"\\t\" + str(tpos) + \"\\n\")\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the training, validation and test files with masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask_type(input_file, output_file):\n",
    "    filename = input_file.split('\\\\')\n",
    "    print(f\"[INFO] Generating masks for {filename[-1]}...\")\n",
    "    with open(output_file, \"w\") as fw:\n",
    "        with open(input_file, \"r\") as fr:\n",
    "            for line in fr.readlines():\n",
    "                fw.write(line.strip('\\r \\n') + \"\\tMASK_HEAD\\n\")\n",
    "                fw.write(line.strip('\\r \\n') + \"\\tMASK_TAIL\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data\\\\fb15k\"\n",
    "\n",
    "# existing (input) files\n",
    "old_train = os.path.join(os.getcwd(), data, \"train.txt\")\n",
    "old_valid = os.path.join(os.getcwd(), data, \"valid.txt\")\n",
    "old_test = os.path.join(os.getcwd(), data, \"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new vocabulary file\n",
    "vocab_file = os.path.join(os.getcwd(), data, \"trial_vocab.txt\")\n",
    "\n",
    "# write all the triples \n",
    "triples_file = os.path.join(os.getcwd(), data, \"trial_all_triples.txt\")\n",
    "\n",
    "# generate masks for the data\n",
    "new_train = os.path.join(os.getcwd(), data, \"trial_train.coke.txt\")\n",
    "new_valid = os.path.join(os.getcwd(), data, \"trial_valid.coke.txt\")\n",
    "new_test = os.path.join(os.getcwd(), data, \"trial_test.coke.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbc_data_preprocess(old_train, old_valid, old_test, \n",
    "                        vocabulary, triples_file, \n",
    "                        new_train, new_valid, new_test):\n",
    "    print(\"[INFO] Initiating data preprocessing...\")\n",
    "    \n",
    "    entity_list, relation_list = get_unique_entities_relations(old_train, old_valid, old_test)\n",
    "    \n",
    "    write_vocab(vocabulary, entity_list, relation_list)\n",
    "    vocab = load_vocab(vocabulary)\n",
    "    \n",
    "    write_true_triples(old_train, old_valid, old_test, vocab, triples_file)\n",
    "\n",
    "    generate_mask_type(old_train, new_train)\n",
    "    generate_mask_type(old_valid, new_valid)\n",
    "    generate_mask_type(old_test, new_test)\n",
    "    \n",
    "    print(\"[INFO] Preprocessing successful!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initiating data preprocessing...\n",
      "[INFO] Extracting entities and relations...\n",
      "[INFO] Working with train.txt file\n",
      "[INFO] Working with valid.txt file\n",
      "[INFO] Working with test.txt file\n",
      "[INFO] Creating vocabulary...\n",
      "[INFO] Vocabulary size 16396\n",
      "[INFO] Writing down all the triples...\n",
      "[INFO] Number of true triples: 592213\n",
      "[INFO] Generating masks for train.txt...\n",
      "[INFO] Generating masks for valid.txt...\n",
      "[INFO] Generating masks for test.txt...\n",
      "[INFO] Preprocessing successful!!\n"
     ]
    }
   ],
   "source": [
    "kbc_data_preprocess(old_train, old_valid, old_test, \n",
    "                    vocab_file, triples_file, \n",
    "                    new_train, new_valid, new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Path Query Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pathquery_get_unique_entities_relations(train, valid, test):\n",
    "#     entity_list = {}\n",
    "#     relation_list = {}\n",
    "    \n",
    "#     for input_file in [train, valid, test]:\n",
    "#         with open(input_file, \"r\") as f:\n",
    "#             for line in f.readlines():\n",
    "#                 tokens = line.strip().split(\"\\t\")\n",
    "#                 assert len(tokens) == 3\n",
    "#                 entity_list[tokens[0]] = len(entity_list)\n",
    "#                 entity_list[tokens[2]] = len(entity_list)\n",
    "#                 relations = tokens[1].split(\",\")\n",
    "#                 for relation in relations:\n",
    "#                     relation_list[relation] = len(relation_list)\n",
    "    \n",
    "#     return entity_list, relation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_base_data(old_train, old_valid, old_test,\n",
    "#                      train_base, valid_base, test_base):\n",
    "#     def fil_base(input_file, output_file):\n",
    "#         fout = open(output_file, \"w\")\n",
    "#         base_n = 0\n",
    "#         with open(input_file, \"r\") as f:\n",
    "#             for line in f.readlines():\n",
    "#                 tokens = line.strip().split(\"\\t\")\n",
    "#                 assert len(tokens) == 3\n",
    "#                 relations = tokens[1].split(\",\")\n",
    "#                 if len(relations) == 1:\n",
    "#                     fout.write(line)\n",
    "#                     base_n += 1\n",
    "#         fout.close()\n",
    "#         return base_n\n",
    "\n",
    "#     train_base_n = fil_base(old_train, train_base)\n",
    "#     valid_base_n = fil_base(old_valid, valid_base)\n",
    "#     test_base_n = fil_base(old_test, test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_onlytail_mask_type(input_file, output_file):\n",
    "#     with open(output_file, \"w\") as fw:\n",
    "#         with open(input_file, \"r\") as fr:\n",
    "#             for line in fr.readlines():\n",
    "#                 fw.write(line.strip('\\r \\n') + \"\\tMASK_TAIL\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pathquery_data_preprocess(old_train, old_valid, old_test,\n",
    "#                               vocab_path, sen_candli_file, trivial_sen_file,\n",
    "#                               new_train, new_valid, new_test,\n",
    "#                               train_base, valid_base, test_base):\n",
    "    \n",
    "#     print(\"Extracting unique entities and relations...\")\n",
    "#     entity_list, relation_list = pathquery_get_unique_entities_relations(old_train, old_valid, old_test)\n",
    "    \n",
    "#     print(\"Updating vocabulary...\")\n",
    "#     write_vocab(vocab_path, entity_list, relation_list)\n",
    "    \n",
    "#     filter_base_data(old_train, old_valid, old_test,\n",
    "#                      train_base, valid_base, test_base)\n",
    "    \n",
    "#     generate_mask_type(old_train, new_train)\n",
    "#     generate_onlytail_mask_type(old_valid, new_valid)\n",
    "#     generate_onlytail_mask_type(old_test, new_test)\n",
    "    \n",
    "#     vocab = load_vocab(vocab_path)\n",
    "    \n",
    "# #     generate_eval_files(vocab_path, old_test, \n",
    "# #                         train_base, valid_base, test_base, \n",
    "# #                         sen_candli_file, trivial_sen_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pathquery_datasets = [\"pathqueryFB\", \"pathqueryWN\"]\n",
    "# data = \"data\\\\pathqueryFB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # existing (input) files\n",
    "# old_train = os.path.join(os.getcwd(), data, \"train\")\n",
    "# old_valid = os.path.join(os.getcwd(), data, \"valid\")\n",
    "# old_test = os.path.join(os.getcwd(), data, \"test\")\n",
    "\n",
    "# new_train = os.path.join(os.getcwd(), data, \"train.coke.txt\")\n",
    "# new_valid = os.path.join(os.getcwd(), data, \"valid.coke.txt\")\n",
    "# new_test = os.path.join(os.getcwd(), data, \"test.coke.txt\")\n",
    "\n",
    "# vocab_file = os.path.join(os.getcwd(), data, \"vocab.txt\")\n",
    "# sen_candli_file = os.path.join(os.getcwd(), data, \"sen_candli.txt\")\n",
    "# trivial_sen_file = os.path.join(os.getcwd(), data, \"trivial_sen.txt\")\n",
    "\n",
    "# train_base = os.path.join(os.getcwd(), data, \"train.base.txt\")\n",
    "# valid_base = os.path.join(os.getcwd(), data, \"valid.base.txt\")\n",
    "# test_base = os.path.join(os.getcwd(), data, \"test.base.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pathquery_data_preprocess(old_train, old_valid, old_test,\n",
    "#                               vocab_file, sen_candli_file, trivial_sen_file,\n",
    "#                               new_train, new_valid, new_test,\n",
    "#                               train_base, valid_base, test_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
